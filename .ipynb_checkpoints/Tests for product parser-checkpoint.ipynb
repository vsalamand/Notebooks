{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=912, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Genism word2vec Model Training\n",
    "model = Word2Vec(food_vocab, min_count=1, size=300, workers=4, window=5, sg=0, hs=1)\n",
    "word_vectors = model.wv\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "vocab = list(model.wv.vocab)\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999978656665574"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "ftmodel = FastText(size=4, window=3, min_count=1, bucket=2)\n",
    "ftmodel.build_vocab(sentences=food_vocab)\n",
    "ftmodel.train(sentences=food_vocab, total_examples=len(food_vocab), epochs=10)\n",
    "ftvocab = list(ftmodel.wv.vocab)\n",
    "\n",
    "\n",
    "ftmodel.wv.similarity('blanc', 'blancs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOOD PARSER\n",
    "\n",
    "#matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "#patterns = list(nlp.tokenizer.pipe(foods.name))\n",
    "#matcher.add(\"FOOD_PATTERN\", None, *patterns)\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "products = list(nlp.pipe(data.description))\n",
    "products = list(nlp.pipe(data.clean_description))\n",
    "\n",
    "    \n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = list(nlp.tokenizer.pipe(foods.name))\n",
    "matcher.add(\"FOOD_PATTERN\", None, *patterns)\n",
    "\n",
    "food_parser = []\n",
    "\n",
    "for line in products:\n",
    "    matches = matcher(line)\n",
    "    elements = []\n",
    "    if len(matches) > 0:\n",
    "        for match_id, start, end in matches:\n",
    "            span = line[start:end]\n",
    "            elements.append(span.text)\n",
    "        food_parser.append(max(elements, key=len).lower())\n",
    "    else:\n",
    "        text_blob_object = TextBlob(line.text)\n",
    "        singular_line = ' '.join(text_blob_object.words.singularize())\n",
    "        matches = matcher(tokenizer(singular_line))\n",
    "        if len(matches) > 0:\n",
    "            for match_id, start, end in matches:\n",
    "                span = tokenizer(singular_line)[start:end]\n",
    "                elements.append(span.text)\n",
    "            food_parser.append(max(elements, key=len).lower())\n",
    "        else:\n",
    "            plural_line = ' '.join(text_blob_object.words.pluralize())\n",
    "            matches = matcher(tokenizer(plural_line))\n",
    "            if len(matches) > 0:\n",
    "                for match_id, start, end in matches:\n",
    "                    span = tokenizer(plural_line)[start:end]\n",
    "                    elements.append(span.text)\n",
    "                food_parser.append(max(elements, key=len).lower())\n",
    "            else:\n",
    "                ratios = process.extractOne(line.text.lower(), foods.name)\n",
    "                if len(ratios) > 0 and ratios[1] > 85:\n",
    "                    food_parser.append(ratios[0])\n",
    "                else:\n",
    "                    food_parser.append(\"\")\n",
    "\n",
    "\n",
    "data['food_match'] = food_parser\n",
    "        \n",
    "\n",
    "print(len(data[data.food_match == '']) / len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOOD PARSER IMPROVED WITH SHELVES / FOODCAT MAPPING\n",
    "\n",
    "mapping_categories = pd.read_csv('/Users/vincentsalamand/Downloads/category_mapping.csv')\n",
    "\n",
    "#matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "#patterns = list(nlp.tokenizer.pipe(foods.name))\n",
    "#matcher.add(\"FOOD_PATTERN\", None, *patterns)\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "for index, product_category in mapping_categories.iterrows():\n",
    "    if product_category[\"shelter type\"] == \"shelter_parent\":\n",
    "        filter_data = data[data.shelter_parent == product_category.shelter].clean_description\n",
    "        mapping_cat = mapping_categories[mapping_categories.shelter == product_category.shelter][\"category ids\"]\n",
    "        \n",
    "    elif product_category[\"shelter type\"] == \"shelter_child\":\n",
    "        filter_data = data[data.shelter_child == product_category.shelter].clean_description\n",
    "        mapping_cat = mapping_categories[mapping_categories.shelter == product_category.shelter][\"category ids\"]   \n",
    "        \n",
    "    products = list(nlp.pipe(filter_data))\n",
    "    \n",
    "    mapping = mapping_cat.str.split(',', expand=True)\n",
    "    \n",
    "    if not mapping.isnull().values.any():\n",
    "        cat_ids = [int(ids[1]) for ids in mapping.iteritems()]\n",
    "        related_foods = foods[foods.category_id.isin(cat_ids)]\n",
    "    else:\n",
    "        related_foods = foods\n",
    "    \n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    patterns = list(nlp.tokenizer.pipe(related_foods.name))\n",
    "    matcher.add(\"FOOD_PATTERN\", None, *patterns)\n",
    "    \n",
    "    food_parser = []\n",
    "    \n",
    "    for line in products:\n",
    "        matches = matcher(line)\n",
    "        elements = []\n",
    "        if len(matches) > 0:\n",
    "            for match_id, start, end in matches:\n",
    "                span = line[start:end]\n",
    "                elements.append(span.text)\n",
    "            food_parser.append(max(elements, key=len).lower())\n",
    "        else:\n",
    "            text_blob_object = TextBlob(line.text)\n",
    "            singular_line = ' '.join(text_blob_object.words.singularize())\n",
    "            matches = matcher(tokenizer(singular_line))\n",
    "            if len(matches) > 0:\n",
    "                for match_id, start, end in matches:\n",
    "                    span = tokenizer(singular_line)[start:end]\n",
    "                    elements.append(span.text)\n",
    "                food_parser.append(max(elements, key=len).lower())\n",
    "            else:\n",
    "                plural_line = ' '.join(text_blob_object.words.pluralize())\n",
    "                matches = matcher(tokenizer(plural_line))\n",
    "                if len(matches) > 0:\n",
    "                    for match_id, start, end in matches:\n",
    "                        span = tokenizer(plural_line)[start:end]\n",
    "                        elements.append(span.text)\n",
    "                    food_parser.append(max(elements, key=len).lower())\n",
    "                else:\n",
    "                    ratios = process.extractOne(line.text.lower(), related_foods.name)\n",
    "                    if len(ratios) > 0 and ratios[1] > 85:\n",
    "                        food_parser.append(ratios[0])\n",
    "                    else:\n",
    "                        food_parser.append(\"\")\n",
    "\n",
    "    if product_category[\"shelter type\"] == \"shelter_parent\":\n",
    "        data.loc[data.shelter_parent == product_category.shelter, 'food_match'] = food_parser\n",
    "    elif product_category[\"shelter type\"] == \"shelter_child\":\n",
    "        data.loc[data.shelter_child == product_category.shelter, 'food_match'] = food_parser\n",
    "        \n",
    "\n",
    "print(len(data[data.food_match == '']) / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for foods unmatch in catalog\n",
    "print(len(set(foods.name.str.lower().tolist()) - set(data.food_match.tolist())))\n",
    "set(foods.name.str.lower().tolist()) - set(data.food_match.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.description.str.lower().str.contains(\"avocat\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
