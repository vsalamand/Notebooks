{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup, CData\n",
    "import requests\n",
    "import html\n",
    "import json\n",
    "from time import sleep\n",
    "import time\n",
    "from pandas.io.json import json_normalize\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"/Applications/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['végétarien', 'recettes', 'bienmanger', 'reequilibrage', 'dietetique', 'recettemaison', 'minciravecplaisir',\n",
    "           'repashealthy', 'recetteminceur', 'mangerequilibre', 'instaregime', 'recettefacile', 'recetterapide', 'saladecomposée',\n",
    "           'mangersainement', 'mangerbien', 'instaregimeuse', 'reequilibragealimentaire', 'mangersain', 'recettedujour',\n",
    "           'mangermieux', 'ideerecette', 'cuisinesaine', 'aimetarecette']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3942\n",
      "797\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "for hashtag in hashtags:\n",
    "    driver.get('https://www.instagram.com/explore/tags/'+hashtag)\n",
    "    time.sleep(5) \n",
    "    s0 = 0\n",
    "    s1 = 1.5\n",
    "    for i in range(4):       \n",
    "        Pagelength = driver.execute_script(\"window.scrollTo({}, document.body.scrollHeight/{});\".format(s0, s1))\n",
    "        source = driver.page_source\n",
    "        data=BeautifulSoup(source, 'html.parser')\n",
    "        body = data.find('body')\n",
    "        script = body.find('span')\n",
    "        for link in script.findAll('a'):\n",
    "             if re.match(\"/p\", link.get('href')):\n",
    "                links.append('https://www.instagram.com'+link.get('href'))\n",
    "        #sleep time is required. If you don't use this Instagram may interrupt the script and doesn't scroll through pages\n",
    "        time.sleep(5)\n",
    "        s0 = s1\n",
    "        s1 = (s1 + 1.5)\n",
    "\n",
    "print(len(links))\n",
    "print(len(set(links)))\n",
    "links = set(links)\n",
    "links = list(links)    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621\n",
      "457\n"
     ]
    }
   ],
   "source": [
    "scraper_result = pd.DataFrame()\n",
    "for i in range(len(links)):\n",
    "    try:\n",
    "        page = urlopen(links[i]).read()\n",
    "        data = BeautifulSoup(page, 'html.parser')\n",
    "        body = data.find('body')\n",
    "        script = body.find('script')\n",
    "        raw = script.text.strip().replace('window._sharedData =', '').replace(';', '')\n",
    "        json_data = json.loads(raw)\n",
    "        if json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['is_video'] == False:\n",
    "            post = json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_to_caption']['edges'][0]['node']\n",
    "            post = json.dumps(post)\n",
    "            post = json.loads(post)\n",
    "            post = json_normalize(post) \n",
    "            post['caption'] = json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['accessibility_caption']\n",
    "            post['timestamp'] = json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['taken_at_timestamp']\n",
    "            post['username'] = json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['owner']['username']\n",
    "            post['likes_count'] = json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['edge_media_preview_like']['count'] \n",
    "            post['link'] = links[i]\n",
    "            post['image_link'] = json_data['entry_data']['PostPage'][0]['graphql']['shortcode_media']['display_url']\n",
    "            x = pd.DataFrame.from_dict(post, orient='columns') \n",
    "            x.columns =  x.columns.str.replace(\"shortcode_media.\", \"\")\n",
    "            scraper_result = scraper_result.append(x)\n",
    "       \n",
    "    except:\n",
    "        np.nan\n",
    "#Just check for the duplicates\n",
    "scraper_result.index = range(len(scraper_result.index))\n",
    "\n",
    "print(len(scraper_result))\n",
    "print(len(scraper_result.loc[scraper_result['caption'].str.contains(\"food\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with posts from the scraper\n",
    "food_posts = scraper_result[scraper_result['caption'].str.contains(\"food\")].copy()\n",
    "food_posts['hashtags'] = food_posts.text.apply(lambda x: re.findall(r'[#@][^\\s#@]+', x))\n",
    "print(len(food_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean posts to prepare for text classification\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "clean_texts = []\n",
    "for index, row in food_posts.iterrows():\n",
    "    clean_post = row.text\n",
    "    # Remove hashtags\n",
    "    hashtags = [hashtag[1:] for hashtag in row.hashtags]\n",
    "    clean_post = ', '.join(str(word) for word in [w for w in clean_post.split('#') if w.strip() not in hashtags])   \n",
    "    # Convert to unicode and remove accents\n",
    "    clean_post = unidecode.unidecode(clean_post)\n",
    "    # Remove Emails\n",
    "    clean_post = re.sub('\\S*@\\S*\\s?', \"\", str(clean_post))\n",
    "    # Remove new line characters\n",
    "    #clean_post = re.sub('\\s+', \" \", str(clean_post))\n",
    "    # Remove distracting single quotes\n",
    "    clean_post = re.sub(\"\\'\", \"\", str(clean_post))\n",
    "    # Remove return line characters\n",
    "    #clean_post = re.sub('\\n', \" \", str(clean_post))\n",
    "    # Remove non alphabetical characters\n",
    "    #clean_post = re.sub(r'[^A-Za-z]', \" \", str(clean_post))\n",
    "    # Lower all characters\n",
    "    clean_post = str(clean_post).lower()\n",
    "    \n",
    "    clean_texts.append(clean_post)\n",
    "    \n",
    "\n",
    "food_posts = food_posts.assign(clean_text=clean_texts)\n",
    "food_posts = food_posts[food_posts.clean_text != '']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save posts in a csv file\n",
    "\n",
    "with open('instagram_food_posts.csv', 'w') as f:        \n",
    "    food_posts.to_csv(f, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of posts with a link to the recipe in user's bio\n",
    "patterns =['ma bio', 'notre bio', 'en bio', 'la bio']\n",
    "indexes = []\n",
    "\n",
    "for index, row in food_posts.iterrows():\n",
    "    for pattern in patterns:\n",
    "        if pattern in row.text:\n",
    "            indexes.append(index)\n",
    "\n",
    "link_posts = food_posts.loc[indexes]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
